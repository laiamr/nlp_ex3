{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To use parquet to load the HuggingFace data into a pandas dataframe, we need to install 3 dependencies: \n",
    "# pyarrow, fastparquet and huggingface_hub (no need to import them, just install them, they are dependency packages to pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'comment', 'post', 'follow', 'tweet', 'agradar', 'comentari', 'comentar', 'publicació', 'publicar', 'seguir', 'tuitar', 'tuitejar', 'tuit', 'piular', 'piulada', 'piulet', 'gustar', 'tuitear']\n"
     ]
    }
   ],
   "source": [
    "# Load the terms of the keywords from the txt file\n",
    "filename = 'SocialMediaTerms.txt'\n",
    "with open(filename, 'r') as file:\n",
    "        text = file.read()\n",
    "terms = text.splitlines()\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENG terms: like, comment, post, follow, tweet\n",
      "CAT/SPA terms: agradar, comentari, comentar, publicació, publicar, seguir, tuitar, tuitejar, tuit, piular, piulada, piulet, gustar, tuitear\n"
     ]
    }
   ],
   "source": [
    "# Split list of terms into English and Catalan/Spanish (some tokens are equal in both languages)\n",
    "eng_terms = terms[:5]\n",
    "cat_sp_terms = terms[5:]\n",
    "print(f\"ENG terms: {', '.join(eng_terms)}\")\n",
    "print(f\"CAT/SPA terms: {', '.join(cat_sp_terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laia/VS Workspace/NLP/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load HuggingFace dataset into pandas dataframe\n",
    "df_corpus = pd.read_parquet(\"hf://datasets/projecte-aina/CaSERa-catalan-stance-emotions-raco/data/train-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_conversation</th>\n",
       "      <th>id_reply</th>\n",
       "      <th>parent_text</th>\n",
       "      <th>reply_text</th>\n",
       "      <th>dynamic_stance</th>\n",
       "      <th>parent_emotion</th>\n",
       "      <th>reply_emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9310554</td>\n",
       "      <td>9310554_2_1</td>\n",
       "      <td>S'ha aturat l'activitat parlamentària. Hi ha d...</td>\n",
       "      <td>Theresa May diu que l'atacant era de nacionali...</td>\n",
       "      <td>2</td>\n",
       "      <td>[no emotion, fear]</td>\n",
       "      <td>[surprise, anger, distrust]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>626929</td>\n",
       "      <td>626929_1_1</td>\n",
       "      <td>Hola a tots. El Rally Lisboa-Dakar està a punt...</td>\n",
       "      <td>Males notícies en el Dakar, ja que ha mort un ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[anticipation, sadness, joy]</td>\n",
       "      <td>[sadness, disgust]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1151534</td>\n",
       "      <td>1151534_2_1</td>\n",
       "      <td>Us heu trobat algu cop algun castellanoparlant...</td>\n",
       "      <td>Si a TV3 es sentíssin totes les varietats de l...</td>\n",
       "      <td>2</td>\n",
       "      <td>[anticipation, anger, joy]</td>\n",
       "      <td>[sadness, anger]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>310430</td>\n",
       "      <td>310430_1_1</td>\n",
       "      <td>Com és pot apreciar i segons dades de la Duana...</td>\n",
       "      <td>Com voleu que la gent no begui cava??? Sí és u...</td>\n",
       "      <td>2</td>\n",
       "      <td>[no emotion, anger]</td>\n",
       "      <td>[disgust, distrust, joy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2995973</td>\n",
       "      <td>2995973_1_1</td>\n",
       "      <td>Ara mateix estic pensant en allò que agrada ta...</td>\n",
       "      <td>En Carretero ho diu ben clar, bé, de fet ja va...</td>\n",
       "      <td>2</td>\n",
       "      <td>[anger, fear, distrust]</td>\n",
       "      <td>[anger, joy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_conversation     id_reply  \\\n",
       "0         9310554  9310554_2_1   \n",
       "1          626929   626929_1_1   \n",
       "2         1151534  1151534_2_1   \n",
       "3          310430   310430_1_1   \n",
       "4         2995973  2995973_1_1   \n",
       "\n",
       "                                         parent_text  \\\n",
       "0  S'ha aturat l'activitat parlamentària. Hi ha d...   \n",
       "1  Hola a tots. El Rally Lisboa-Dakar està a punt...   \n",
       "2  Us heu trobat algu cop algun castellanoparlant...   \n",
       "3  Com és pot apreciar i segons dades de la Duana...   \n",
       "4  Ara mateix estic pensant en allò que agrada ta...   \n",
       "\n",
       "                                          reply_text  dynamic_stance  \\\n",
       "0  Theresa May diu que l'atacant era de nacionali...               2   \n",
       "1  Males notícies en el Dakar, ja que ha mort un ...               4   \n",
       "2  Si a TV3 es sentíssin totes les varietats de l...               2   \n",
       "3  Com voleu que la gent no begui cava??? Sí és u...               2   \n",
       "4  En Carretero ho diu ben clar, bé, de fet ja va...               2   \n",
       "\n",
       "                 parent_emotion                reply_emotion  \n",
       "0            [no emotion, fear]  [surprise, anger, distrust]  \n",
       "1  [anticipation, sadness, joy]           [sadness, disgust]  \n",
       "2    [anticipation, anger, joy]             [sadness, anger]  \n",
       "3           [no emotion, anger]     [disgust, distrust, joy]  \n",
       "4       [anger, fear, distrust]                 [anger, joy]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize data structure\n",
    "df_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge text from 2 columns in the DF: parent_text and reply_text into a single pandas dataframe\n",
    "posts_df = pd.DataFrame(df_corpus['parent_text'] + df_corpus['reply_text'], columns=['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13999, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S'ha aturat l'activitat parlamentària. Hi ha d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hola a tots. El Rally Lisboa-Dakar està a punt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Us heu trobat algu cop algun castellanoparlant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Com és pot apreciar i segons dades de la Duana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ara mateix estic pensant en allò que agrada ta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  S'ha aturat l'activitat parlamentària. Hi ha d...\n",
       "1  Hola a tots. El Rally Lisboa-Dakar està a punt...\n",
       "2  Us heu trobat algu cop algun castellanoparlant...\n",
       "3  Com és pot apreciar i segons dades de la Duana...\n",
       "4  Ara mateix estic pensant en allò que agrada ta..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See new dataframe with only the text of the forum's posts\n",
    "print(posts_df.shape)\n",
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Catalan model in spacy - to tokenise and lemmatise\n",
    "nlp = spacy.load(\"ca_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tags we want to keep in the texts - Noun, Verb, Proper Noun and 'Other' (in case the tagger does not know what to do with the English words)\n",
    "pos_keep = ['NOUN', 'VERB', 'PROPN', 'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to tokenize/lemmatise a string of text and\n",
    "# return the text with the lemmas (create a string from joining all lemmas by space)\n",
    "# this is to make it easier to use the regex package to find matches\n",
    "def tokenise_lemmatise_text(sentence: str) -> str:\n",
    "    ''' Given a string of text, the function tokenises it and lemmatises\n",
    "    it with a Catalan language pipeline\n",
    "    sentence -> the text we want to process\n",
    "    return -> a string with each element lemmatised. Lemmas separated by space\n",
    "    '''\n",
    "    # Apply the spacy pipeline to the lowercase text\n",
    "    doc = nlp(sentence.lower())\n",
    "    # Create a list with all the lemmas with the POS we want\n",
    "    lemmas = [w.lemma_ for w in doc if w.pos_ in pos_keep]\n",
    "    # Convert into a string for easier manipulation\n",
    "    text = \" \".join(lemmas)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find our keywords in the lemmatised texts\n",
    "def search_keywords(text: str, wordlist: list) -> str:\n",
    "    ''' Given a string of text, we try to find if it contains any of\n",
    "    the keywords defined in the terms list\n",
    "    text -> the text we want to search through\n",
    "    wordlist -> the list of words we want to search for\n",
    "    return -> a string with all the terms found in the text, separated by comma. If none is found, returns \"NA\"\n",
    "    '''\n",
    "    found_terms = []\n",
    "    # Iterate over all the keyword terms from the datafile\n",
    "    for word in wordlist:\n",
    "        # See if a match is found in the text -> if so, add to list\n",
    "        if re.search(word, text, flags=re.IGNORECASE) != None:\n",
    "            found_terms.append(word)\n",
    "            \n",
    "    # Convert to string for easier manipulation and to unify return types\n",
    "    return \", \".join(found_terms) if len(found_terms) > 0 else \"NA\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split strings (rows) with multiple terms -> set to a new list\n",
    "def split_strings_list(wordlist: list) -> list:\n",
    "    ''' Given a list of strings, it splits the contents of each string into a list using the\n",
    "    separator \", \" defined previously.\n",
    "    wordlist -> list of strings with list-like format, separated by comma and space\n",
    "    return -> returns a list of all the individual words\n",
    "    '''\n",
    "    split_strings = []\n",
    "    for item in wordlist:\n",
    "        temp = item.split(', ') # Split the string by the previously defined separator: \", \"\n",
    "        split_strings.extend(temp) # Add result on the same level (no list of lists)\n",
    "    return split_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the tokeniser-lemmatiser function to all rows of the dataframe\n",
    "posts_df['Lemmatised'] = posts_df['Text'].apply(lambda row: tokenise_lemmatise_text(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the search function to all rows of the dataframe to see which ones contain any of our keywords\n",
    "# First we apply with the English terms\n",
    "posts_df['English_keywords'] = posts_df['Lemmatised'].apply(lambda row: search_keywords(row, eng_terms))\n",
    "# Then we apply with the Catalan/Spanish terms\n",
    "posts_df['Cat_Sp_keywords'] = posts_df['Lemmatised'].apply(lambda row: search_keywords(row, cat_sp_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Lemmatised</th>\n",
       "      <th>English_keywords</th>\n",
       "      <th>Cat_Sp_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S'ha aturat l'activitat parlamentària. Hi ha d...</td>\n",
       "      <td>aturar activitat haver video oficial commotion...</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hola a tots. El Rally Lisboa-Dakar està a punt...</td>\n",
       "      <td>holar rally lisboa-dakar punt acabar pilot mar...</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Us heu trobat algu cop algun castellanoparlant...</td>\n",
       "      <td>trobar cop castellanoparlant dir entendre parl...</td>\n",
       "      <td>NA</td>\n",
       "      <td>seguir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Com és pot apreciar i segons dades de la Duana...</td>\n",
       "      <td>apreciar dada duana cava perjudicar declaració...</td>\n",
       "      <td>NA</td>\n",
       "      <td>agradar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ara mateix estic pensant en allò que agrada ta...</td>\n",
       "      <td>pensar agradar pp empadronar persona casa fer ...</td>\n",
       "      <td>NA</td>\n",
       "      <td>agradar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  S'ha aturat l'activitat parlamentària. Hi ha d...   \n",
       "1  Hola a tots. El Rally Lisboa-Dakar està a punt...   \n",
       "2  Us heu trobat algu cop algun castellanoparlant...   \n",
       "3  Com és pot apreciar i segons dades de la Duana...   \n",
       "4  Ara mateix estic pensant en allò que agrada ta...   \n",
       "\n",
       "                                          Lemmatised English_keywords  \\\n",
       "0  aturar activitat haver video oficial commotion...               NA   \n",
       "1  holar rally lisboa-dakar punt acabar pilot mar...               NA   \n",
       "2  trobar cop castellanoparlant dir entendre parl...               NA   \n",
       "3  apreciar dada duana cava perjudicar declaració...               NA   \n",
       "4  pensar agradar pp empadronar persona casa fer ...               NA   \n",
       "\n",
       "  Cat_Sp_keywords  \n",
       "0              NA  \n",
       "1              NA  \n",
       "2          seguir  \n",
       "3         agradar  \n",
       "4         agradar  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1333, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter rows of dataframe that contain English terms\n",
    "posts_df[posts_df['English_keywords'] != \"NA\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Lemmatised</th>\n",
       "      <th>English_keywords</th>\n",
       "      <th>Cat_Sp_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Davant la possibilitat realista que algun dels...</td>\n",
       "      <td>possibilitat pig acrònim portugal italy greece...</td>\n",
       "      <td>post</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Text rebut per correu electrònic (perdoneu la ...</td>\n",
       "      <td>text correu perdonar llargada oració mare déu ...</td>\n",
       "      <td>post</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Desrprés de tot l'escàndol que ha portat aques...</td>\n",
       "      <td>escàndol portar vídeo baixa agradar saber opin...</td>\n",
       "      <td>post</td>\n",
       "      <td>agradar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Entrevista a Arnaldo Otegi al diari Gara: \"L'E...</td>\n",
       "      <td>entrevista arnaldo otegi diari gara estat assu...</td>\n",
       "      <td>post</td>\n",
       "      <td>publicar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Aldarulls després de la derrota dels Vancouver...</td>\n",
       "      <td>aldarull derrota vancouver canucks boston part...</td>\n",
       "      <td>post</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  \\\n",
       "9   Davant la possibilitat realista que algun dels...   \n",
       "18  Text rebut per correu electrònic (perdoneu la ...   \n",
       "23  Desrprés de tot l'escàndol que ha portat aques...   \n",
       "32  Entrevista a Arnaldo Otegi al diari Gara: \"L'E...   \n",
       "38  Aldarulls després de la derrota dels Vancouver...   \n",
       "\n",
       "                                           Lemmatised English_keywords  \\\n",
       "9   possibilitat pig acrònim portugal italy greece...             post   \n",
       "18  text correu perdonar llargada oració mare déu ...             post   \n",
       "23  escàndol portar vídeo baixa agradar saber opin...             post   \n",
       "32  entrevista arnaldo otegi diari gara estat assu...             post   \n",
       "38  aldarull derrota vancouver canucks boston part...             post   \n",
       "\n",
       "   Cat_Sp_keywords  \n",
       "9               NA  \n",
       "18              NA  \n",
       "23         agradar  \n",
       "32        publicar  \n",
       "38              NA  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df[posts_df['English_keywords'] != \"NA\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'post': 1300, 'like': 25, 'comment': 9, 'follow': 7, 'tweet': 4})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the English keywords found and see which ones appear and with which frequency\n",
    "eng_results = posts_df[posts_df['English_keywords'] != \"NA\"]['English_keywords'].tolist() # Filter column (no \"NA\") and cast to list\n",
    "eng_found_terms = split_strings_list(eng_results) # Split the rows that contain more than one keyword\n",
    "Counter(eng_found_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3774, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter rows of dataframe that contain Catalan or Spanish terms\n",
    "posts_df[posts_df['Cat_Sp_keywords'] != \"NA\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Lemmatised</th>\n",
       "      <th>English_keywords</th>\n",
       "      <th>Cat_Sp_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Davant la possibilitat realista que algun dels...</td>\n",
       "      <td>possibilitat pig acrònim portugal italy greece...</td>\n",
       "      <td>post</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Text rebut per correu electrònic (perdoneu la ...</td>\n",
       "      <td>text correu perdonar llargada oració mare déu ...</td>\n",
       "      <td>post</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Desrprés de tot l'escàndol que ha portat aques...</td>\n",
       "      <td>escàndol portar vídeo baixa agradar saber opin...</td>\n",
       "      <td>post</td>\n",
       "      <td>agradar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Entrevista a Arnaldo Otegi al diari Gara: \"L'E...</td>\n",
       "      <td>entrevista arnaldo otegi diari gara estat assu...</td>\n",
       "      <td>post</td>\n",
       "      <td>publicar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Aldarulls després de la derrota dels Vancouver...</td>\n",
       "      <td>aldarull derrota vancouver canucks boston part...</td>\n",
       "      <td>post</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  \\\n",
       "9   Davant la possibilitat realista que algun dels...   \n",
       "18  Text rebut per correu electrònic (perdoneu la ...   \n",
       "23  Desrprés de tot l'escàndol que ha portat aques...   \n",
       "32  Entrevista a Arnaldo Otegi al diari Gara: \"L'E...   \n",
       "38  Aldarulls després de la derrota dels Vancouver...   \n",
       "\n",
       "                                           Lemmatised English_keywords  \\\n",
       "9   possibilitat pig acrònim portugal italy greece...             post   \n",
       "18  text correu perdonar llargada oració mare déu ...             post   \n",
       "23  escàndol portar vídeo baixa agradar saber opin...             post   \n",
       "32  entrevista arnaldo otegi diari gara estat assu...             post   \n",
       "38  aldarull derrota vancouver canucks boston part...             post   \n",
       "\n",
       "   Cat_Sp_keywords  \n",
       "9               NA  \n",
       "18              NA  \n",
       "23         agradar  \n",
       "32        publicar  \n",
       "38              NA  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df[posts_df['English_keywords'] != \"NA\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'seguir': 1612,\n",
       "         'agradar': 1539,\n",
       "         'comentar': 707,\n",
       "         'comentari': 376,\n",
       "         'publicar': 239,\n",
       "         'publicació': 66,\n",
       "         'tuit': 42,\n",
       "         'piulada': 38,\n",
       "         'gustar': 9,\n",
       "         'tuitar': 6,\n",
       "         'tuitejar': 4,\n",
       "         'piular': 4,\n",
       "         'piulet': 2})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the Catalan/Spanish keywords found and see which ones appear and with which frequency\n",
    "cat_sp_results = posts_df[posts_df['Cat_Sp_keywords'] != \"NA\"]['Cat_Sp_keywords'].tolist() # Filter column (no \"NA\") and cast to list\n",
    "cat_sp_found_terms = split_strings_list(cat_sp_results) # Split the rows that contain more than one keyword\n",
    "Counter(cat_sp_found_terms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
